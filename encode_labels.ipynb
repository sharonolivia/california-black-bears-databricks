{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "66f5e77a-73b3-4115-9d87-04e5c3816d15",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "<b>MSDS697 Spring I 2023 - Final Project</b><br>\n",
    "<b>Group 15</b> - Project Bears<br>\n",
    "<b>Members</b> - Sharon Dodda, Ensun Pak\n",
    "\n",
    "This notebook is only used to encode the labels of the training dataset. This cannot be part of the ML pipeline, because we are taking only the historical data as training data and to encode the data with NLCD takes too long to be practical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1577e4f2-e66e-4b7e-ac90-224dde1b91c7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.databricks.v1+bamboolib_hint": "{\"pd.DataFrames\": [], \"version\": \"0.0.1\"}",
      "text/plain": []
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": {
        "application/vnd.databricks.v1+bamboolib_hint": "{\"pd.DataFrames\": [], \"version\": \"0.0.1\"}",
        "text/plain": ""
       },
       "datasetInfos": [],
       "executionCount": null,
       "metadata": {
        "kernelSessionId": "8504f3c8-7f6a5f8221942758c76f8834"
       },
       "removedWidgets": [],
       "type": "mimeBundle"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import packages\n",
    "from pyspark.sql.functions import *\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1619b819-2479-4458-9463-6577a14998a9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Initialize parameters to connect to MongoDB Atlas\n",
    "mongo_username = os.environ.get(\"MONGO_USERNAME\")\n",
    "mongo_password =  os.environ.get(\"MONGO_PASSWORD\")\n",
    "mongo_ip_address = os.environ.get(\"MONGO_IP\")\n",
    "database_name = os.environ.get(\"MONGO_DB_NAME\")\n",
    "\n",
    "connectionString=f\"mongodb+srv://{mongo_username}:{mongo_password}@{mongo_ip_address}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ddfeb5dd-3c5d-41aa-ab23-e219f608157e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Process base training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "34f54f01-58fd-46a6-9a2d-6a29a0b05732",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Get the bears collections from MongoDB\n",
    "database=\"msds697_bears\"\n",
    "collection=\"inat_historical\"\n",
    "\n",
    "inat_historical = spark.read.format(\"mongo\")\\\n",
    "                    .option(\"database\", database)\\\n",
    "                    .option(\"spark.mongodb.input.uri\", connectionString)\\\n",
    "                    .option(\"collection\", collection).load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af420f51-3b87-4404-9ebb-9fff83a616b1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Reorganize layout of pyspark df\n",
    "inat_historical = inat_historical.withColumn('lat', col('coords')[0]).withColumn('lon', col('coords')[1])\n",
    "bears_df = inat_historical.select('coords',\n",
    "                                  'county',\n",
    "                                  'observed_period.date_unit.day',\n",
    "                                  'observed_period.date_unit.month',\n",
    "                                  'observed_period.date_unit.year')\n",
    "\n",
    "# Convert pyspark DF to pandas DF\n",
    "bears_df = bears_df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b3a32323-7f9d-4b53-8260-869b21a2f417",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def dist_btw_2_coords(coord1, coord2):\n",
    "    \"\"\"\n",
    "    Calculates the Euclidean distance between two coordinate points.\n",
    "    \"\"\"\n",
    "    from math import sin, cos, sqrt, atan2, radians\n",
    "    \n",
    "    # Approximate radius of earth in km\n",
    "    R = 6373.0\n",
    "\n",
    "\n",
    "    lat1 = radians(coord1[0])\n",
    "    lon1 = radians(coord1[1])\n",
    "    lat2 = radians(coord2[0])\n",
    "    lon2 = radians(coord2[1])\n",
    "\n",
    "    dlon = lon2 - lon1\n",
    "    dlat = lat2 - lat1\n",
    "\n",
    "    a = sin(dlat / 2)**2 + cos(lat1) * cos(lat2) * sin(dlon / 2)**2\n",
    "    c = 2 * atan2(sqrt(a), sqrt(1 - a))\n",
    "\n",
    "    distance_in_miles = R * c * 0.621371\n",
    "\n",
    "    # print(\"Result: \", distance_in_miles)\n",
    "    \n",
    "    return distance_in_miles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "95519b6d-bf0f-4a60-92a9-94da0fc3a35c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Calculate the target label for more than 1 bears sighted within 2 miles of each other\n",
    "bears_df['label'] = ''\n",
    "for i in range(bears_df.shape[0]):\n",
    "    if (i < bears_df.shape[0]):\n",
    "        for j in range(i+1, bears_df.shape[0]):\n",
    "            dist = dist_btw_2_coords(bears_df.iloc[i,0], bears_df.iloc[j,0])\n",
    "            if dist <= 2:\n",
    "                bears_df.iloc[i,5] = 1\n",
    "                break\n",
    "        else:\n",
    "            bears_df.iloc[i,5] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a0537861-efc9-47c6-ae42-9bf33e2680de",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Extract latitude and longitude data, and truncate their decimal points to 5 spaces\n",
    "bears_df['lat'] = bears_df.apply(lambda x: int(x['coords'][0] * 100000) / 100000, axis=1)\n",
    "bears_df['lon'] = bears_df.apply(lambda x: int(x['coords'][1] * 100000) / 100000, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8dbb1432-90d9-4c99-920c-6fe705a99a76",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Add NLCD land code to the historical training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "55f087f0-5d94-457c-bcd8-340a4d44c5ae",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Get the bears collections from MongoDB\n",
    "database=\"msds697_bears\"\n",
    "collection=\"nlcd_ca_data\"\n",
    "\n",
    "nlcd_ca = spark.read.format(\"mongo\")\\\n",
    "                    .option(\"database\", database)\\\n",
    "                    .option(\"spark.mongodb.input.uri\", connectionString)\\\n",
    "                    .option(\"collection\", collection).load()\n",
    "\n",
    "# Reorganize layout of pyspark df\n",
    "nlcd_ca = nlcd_ca.select('lat', 'lon', 'code')\n",
    "\n",
    "# Convert to pandas df\n",
    "nlcd_ca = nlcd_ca.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "01c64217-f5ce-4054-b3e0-acabd3036c8a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " bears_df2 row 1304\n",
      "\n",
      "*** WARNING: max output size exceeded, skipping output. ***\n",
      "\n",
      " bears_df2 row 5544\r"
     ]
    }
   ],
   "source": [
    "# Add land code to the training set (brute force method)\n",
    "# Processing whole dataset will take about 1 hour\n",
    "bears_df2 = bears_df.copy()\n",
    "bears_df2['code'] = '99'\n",
    "for i in range(bears_df2.shape[0]):\n",
    "    bear_coord = bears_df2.iloc[i, 6:8].values\n",
    "    \n",
    "    temp_nlcd = nlcd_ca.copy()\n",
    "    temp_nlcd['dist'] = temp_nlcd.apply(lambda x: dist_btw_2_coords(bear_coord, [x['lat'], x['lon']]), axis=1)\n",
    "    code = temp_nlcd.sort_values('dist').head(1).iloc[:, 2].values[0]\n",
    "    bears_df2.iloc[i, 8] = code\n",
    "    print(f\" bears_df2 row {i}\", end=\"\\r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f4ad8866-a5fb-477d-a068-3aef64be11e3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Convert the pandas df back to pyspark df and push it to MongoDB for storage\n",
    "bears = spark.createDataFrame(bears_df2)\n",
    "\n",
    "bears.write.format(\"mongo\")\\\n",
    "        .option(\"spark.mongodb.output.uri\", connectionString)\\\n",
    "        .option(\"database\", database)\\\n",
    "        .option(\"collection\", \"training_data\")\\\n",
    "        .mode(\"append\").save()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "msds697_grp15_encode_labels",
   "notebookOrigID": 4205848236188060,
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

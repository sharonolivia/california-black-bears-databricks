{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c792871d-0fdd-4441-ac0a-46b200fd90b4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## <b>MSDS697 Spring I 2023 - Final Project</b><br>\n",
    "<b>Group 15</b> - Project Bears<br>\n",
    "<b>Members</b> - Sharon Dodda, Ensun Pak\n",
    "\n",
    "This notebook will pull the training data from MongoDB. There will be some light pre-processing on the data prior to using it for model training. We setup a simple pipeline to convert the county feature into a numerical column, assemble the features into a vector via VectorAssembler. The model is cross-validated on 5 folds to tune the model hyperparameters. The model performance will be measured using the area under ROC.\n",
    "\n",
    "<b>Analytic Objective</b><br>\n",
    "To predict if a bear will appear again in the areas where there was bear sighting in the past.\n",
    "\n",
    "<b>Problem Type</b><br>\n",
    "This will be a binary classification problem where a logistic regression model will be trained to predict future bear sightings.\n",
    "\n",
    "<b>Conclusion</b><br>\n",
    "After cross-validating the training set, the logistic regression model achieved an area under ROC score of 0.6471. Evaluating this model on the test set, it achieved an area under ROC score of 0.6503. While the model performs only moderately better than random, the higher area under ROC score in the test evaluation suggests that the model is able to generalize new data well.<br>\n",
    "\n",
    "<b>The best model has the following tuned hyperparameters:</b><br>\n",
    "regParam: 500<br> \n",
    "maxIters: 0.5<br>\n",
    "elasticNet: 0.25\n",
    "\n",
    "<b>Model training performance summary:</b><br>\n",
    "Training dataset size: 1,982 observations; 10 features<br>\n",
    "Pipeline execution time: 52 minutes<br>\n",
    "Train evaluation execution time: 6 seconds<br>\n",
    "Test evaluation execution time: 2 seconds<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "88e454cc-aa8c-43bc-9d2b-ec5c0a32c8a1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Import packages\n",
    "from pyspark.sql.functions import *\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "efbf14d8-a9a2-4eae-9499-914c71488279",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Initialize MongoDB link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f3ebc6d3-e9fc-4d42-9d3a-7817fc9939d9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Initialize parameters to connect to MongoDB Atlas\n",
    "mongo_username = os.environ.get(\"MONGO_USERNAME\")\n",
    "mongo_password =  os.environ.get(\"MONGO_PASSWORD\")\n",
    "mongo_ip_address = os.environ.get(\"MONGO_IP\")\n",
    "database_name = os.environ.get(\"MONGO_DB_NAME\")\n",
    "\n",
    "connectionString=f\"mongodb+srv://{mongo_username}:{mongo_password}@{mongo_ip_address}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e9cf8a8a-7e65-4399-8fbf-ac0b8f5efc5b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Get training data from MongoDB for pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f94ed611-ecb2-46c0-a7a5-9ce90878455e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Grab historical weather data from MongoDB\n",
    "database=\"msds697_bears\"\n",
    "collection=\"noaa_weather_historical\"\n",
    "\n",
    "noaa_hist = spark.read.format(\"mongo\")\\\n",
    "                    .option(\"database\", database)\\\n",
    "                    .option(\"spark.mongodb.input.uri\", connectionString)\\\n",
    "                    .option(\"collection\", collection).load()\n",
    "\n",
    "# Oldest date in training set is June 26, 1988\n",
    "# Get tmax data\n",
    "noaa_hist_tmax = noaa_hist.filter(\"datatype == 'TMAX'\")\\\n",
    "                          .select('county', col('date').cast('date'), 'value')\\\n",
    "                          .withColumnRenamed('value', 'tmax')\n",
    "\n",
    "# Get tmin data\n",
    "noaa_hist_tmin = noaa_hist.filter(\"datatype == 'TMIN'\")\\\n",
    "                          .select('county', col('date').cast('date'), 'value')\\\n",
    "                          .withColumnRenamed('value', 'tmin')\n",
    "\n",
    "# Get prcp data\n",
    "noaa_hist_prcp = noaa_hist.filter(\"datatype == 'PRCP'\")\\\n",
    "                          .select('county', col('date').cast('date'), 'value')\\\n",
    "                          .withColumnRenamed('value', 'prcp')\n",
    "\n",
    "# Join the data into one df\n",
    "noaa = noaa_hist_tmax.join(noaa_hist_tmin, ['county', 'date'], 'left').join(noaa_hist_prcp, ['county', 'date'], 'left')\n",
    "\n",
    "# Process df\n",
    "noaa = noaa.withColumn('day', dayofmonth('date'))\\\n",
    "           .withColumn('month', month('date'))\\\n",
    "           .withColumn('year', year('date'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1f73105a-f7d6-44ee-b15d-d5f78352325a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Grab training set data from MongoDB\n",
    "database=\"msds697_bears\"\n",
    "collection=\"training_data\"\n",
    "bears = spark.read.format(\"mongo\")\\\n",
    "                    .option(\"database\", database)\\\n",
    "                    .option(\"spark.mongodb.input.uri\", connectionString)\\\n",
    "                    .option(\"collection\", collection).load()\n",
    "\n",
    "# Process df\n",
    "bears = bears.select('day', 'month', 'year', 'county', 'lat', 'lon', 'code', 'label')\n",
    "bears = bears.withColumn('county', regexp_replace('county', ' County', ''))\n",
    "bears = bears.withColumn('date', (concat_ws('-', 'year', 'month', 'day')).cast('date'))\n",
    "\n",
    "# Exclude records before year 2018\n",
    "bears = bears.filter(\"date >= '2018-01-01'\")\n",
    "bears = bears.filter(\"county != 'missing'\")\n",
    "bears = bears.drop('date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "191a0ab4-afb8-4a66-81e5-4cff83cdecd2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Join spark df together\n",
    "bears = bears.join(noaa, ['county', 'year', 'month', 'day'], 'left')\n",
    "\n",
    "# Exclude joined records with no weather data\n",
    "bears = bears.filter(col('date').isNotNull())\n",
    "\n",
    "# Exclude joined records with no prcp data\n",
    "bears = bears.filter(col('prcp').isNotNull())\n",
    "\n",
    "# Exclude joined records with no tmin data\n",
    "bears = bears.filter(col('tmin').isNotNull())\n",
    "\n",
    "# Prepare final df for model fitting\n",
    "bears_data = bears.select('county', 'year', 'month', 'day', 'lat', 'lon', 'code', 'tmax', 'tmin', 'prcp', 'label')\n",
    "\n",
    "bears_data.schema['prcp'].nullable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6af4f3f5-727b-43a5-81a2-b4d73399cbc5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Implement Machine Learning (Model training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "60900b7b-6022-4579-809b-4c31a466b656",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load libraries for machine learning pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.feature import VectorAssembler, StringIndexer, StringIndexerModel\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "da71adc4-a7fe-4699-937d-79334f19bb4d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create train / test datasets\n",
    "bear_data_sets = bears_data.randomSplit([0.8, 0.2])\n",
    "train = bear_data_sets[0].cache()\n",
    "test = bear_data_sets[1].cache()\n",
    "\n",
    "# Prepare stages for pipeline\n",
    "# Stage 1: Convert county from string to numerical with StringIndexer\n",
    "stage_1 = StringIndexer(inputCol='county', outputCol='county_index').setHandleInvalid(\"skip\")\n",
    "\n",
    "# Stage 2: Assemble the features into a feature vector with VectorAssembler\n",
    "stage_2 = VectorAssembler(inputCols=['county_index', 'year', 'month', 'day', 'lat', 'lon', 'code', 'tmax', 'tmin', 'prcp'],\n",
    "                         outputCol='features')\n",
    "\n",
    "# Stage 3: Fit the logistic regression model \n",
    "stage_3 = LogisticRegression(featuresCol='features', labelCol='label')\n",
    "\n",
    "# Construct the pipeline\n",
    "regression_pl = Pipeline(stages=[stage_1, stage_2, stage_3])\n",
    "\n",
    "# Define the evaluator\n",
    "evaluator = BinaryClassificationEvaluator()\n",
    "\n",
    "# Setup hyperparameters to be tuned\n",
    "# paramGrid = ParamGridBuilder().addGrid(stage_3.maxIter, [100, 500, 1000])\\\n",
    "#                               .addGrid(stage_3.regParam, [0.0001, 0.001, 0.01, 0.05, 0.1, 0.25, 0.5, 1])\\\n",
    "#                               .addGrid(stage_3.elasticNetParam, [0, 0.25, 0.5, 0.75, 1] ).build()\n",
    "\n",
    "# Setup hyperparameters to be tuned\n",
    "paramGrid = ParamGridBuilder().addGrid(stage_3.maxIter, [500, 1000])\\\n",
    "                              .addGrid(stage_3.regParam, [0.0001, 0.001, 0.01, 0.05, 0.1, 1])\\\n",
    "                              .addGrid(stage_3.elasticNetParam, [0.1, 0.25, 0.5, 0.75, 1]).build()\n",
    "\n",
    "# Setup cross validator object\n",
    "cv = CrossValidator(estimator=regression_pl,\n",
    "                    evaluator=evaluator,\n",
    "                    numFolds=5,\n",
    "                    estimatorParamMaps=paramGrid)\n",
    "\n",
    "# Fit the model\n",
    "cv_model = cv.fit(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "35c8678b-5542-4139-a1d2-eb49482abe3a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b683abe-56e8-48af-8cc5-debda5ac9501",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[11]: {Param(parent='LogisticRegression_cb81447afa24', name='maxIter', doc='max number of iterations (>= 0).'): 500,\n",
      " Param(parent='LogisticRegression_cb81447afa24', name='regParam', doc='regularization parameter (>= 0).'): 0.05,\n",
      " Param(parent='LogisticRegression_cb81447afa24', name='elasticNetParam', doc='the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.'): 0.25}"
     ]
    }
   ],
   "source": [
    "# Best hyperparameters\n",
    "cv_model.getEstimatorParamMaps()[np.argmax(cv_model.avgMetrics)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1f8dfb64-3867-45f4-9bde-892072e57730",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training evaluation: areaUnderROC : 0.6546\n"
     ]
    }
   ],
   "source": [
    "# Get prediction from best model\n",
    "y_pred = cv_model.bestModel.transform(train)\n",
    "\n",
    "# Evaluate performance - area under ROC\n",
    "print(f\"Training evaluation: {evaluator.getMetricName()} : {evaluator.evaluate(y_pred):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f6624bf6-40fd-4c27-9f04-ad81243b064e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test evaluation: areaUnderROC : 0.6427\n"
     ]
    }
   ],
   "source": [
    "# Evaluate model performance on test set\n",
    "y_hat = cv_model.bestModel.transform(test)\n",
    "\n",
    "# Evaluate performance - area under ROC\n",
    "print(f\"Test evaluation: {evaluator.getMetricName()} : {evaluator.evaluate(y_hat):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a1122524-4b41-4a26-ab1f-b41afe255e6b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+----------+\n",
      "|            features|       rawPrediction|         probability|prediction|\n",
      "+--------------------+--------------------+--------------------+----------+\n",
      "|[15.0,2018.0,6.0,...|[-1.1498621511422...|[0.24051426260845...|       1.0|\n",
      "|[15.0,2018.0,6.0,...|[-1.0476403715788...|[0.25967847160900...|       1.0|\n",
      "|[15.0,2018.0,8.0,...|[-1.0436311803425...|[0.26044996313019...|       1.0|\n",
      "|[15.0,2018.0,10.0...|[-0.8686038022162...|[0.29554490474468...|       1.0|\n",
      "|[15.0,2019.0,5.0,...|[-1.0874722129504...|[0.25209457596919...|       1.0|\n",
      "+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred.select('features', 'rawPrediction', 'probability', 'prediction').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c21795b-c9a9-474e-b57d-2e83e1ba3766",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+----------+\n",
      "|            features|       rawPrediction|         probability|prediction|\n",
      "+--------------------+--------------------+--------------------+----------+\n",
      "|[2.0,2020.0,11.0,...|[-1.4319759290077...|[0.19279099829339...|       1.0|\n",
      "|[0.0,2019.0,5.0,4...|[-1.7772936106343...|[0.14463764015408...|       1.0|\n",
      "|[1.0,2020.0,10.0,...|[-1.4417260379776...|[0.19127820398915...|       1.0|\n",
      "|[12.0,2021.0,10.0...|[-1.0190047043220...|[0.26522131740086...|       1.0|\n",
      "|[15.0,2019.0,8.0,...|[-0.9692136684416...|[0.27503726251059...|       1.0|\n",
      "+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_hat.select('features', 'rawPrediction', 'probability', 'prediction').show(5)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "msds697_grp15_model_training",
   "notebookOrigID": 3451456928320093,
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
